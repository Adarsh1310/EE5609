\documentclass{article}
\usepackage{setspace}
 \usepackage{gensymb}
 \usepackage{graphicx}
 \singlespacing
\graphicspath{ {/user/adarshsrivastava/desktop/Foundation of Machine Learning/Assignment_1} }
 \usepackage[cmex10]{amsmath}
 \usepackage{amsthm}
 \usepackage{hyperref}
 \usepackage{mathrsfs}
 \usepackage{txfonts}
 \usepackage{stfloats}
 \usepackage{bm}
 \usepackage{cite}
 \usepackage{cases}
 \usepackage{subfig}
 \usepackage{longtable}
 \usepackage{multirow}
 \usepackage{enumitem}
 \usepackage{pythontex}
 \usepackage{mathtools}
 \usepackage{steinmetz}
 \usepackage{tikz}
 \usepackage{listings}
 \usepackage{circuitikz}
 \usepackage{verbatim}
 \usepackage{tfrupee}
 \usepackage[breaklinks=true]{hyperref}
 \usepackage{tkz-euclide}
 \usetikzlibrary{calc,math}
 \usepackage{listings}
     \usepackage{color}                                            %%
     \usepackage{array}                                            %%
     \usepackage{longtable}                                        %%
     \usepackage{calc}                                             %%
     \usepackage{multirow}                                         %%
     \usepackage{hhline}                                           %%
     \usepackage{ifthen}                                           %%
     \usepackage{lscape}     
 \usepackage{multicol}
 \usepackage{chngcntr}
 \DeclareMathOperator*{\Res}{Res}
 \renewcommand\thesection{\arabic{section}}
 \renewcommand\thesubsection{\thesection.\arabic{subsection}}
 \renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}
 \renewcommand\thesectiondis{\arabic{section}}
 \renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
 \renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}
 \hyphenation{op-tical net-works semi-conduc-tor}
 \def\inputGnumericTable{}                                 %%
 \lstset{
 %language=C,
 frame=single, 
 breaklines=true,
 columns=fullflexible
 }
 \begin{document}
 \newtheorem{theorem}{Theorem}[section]
 \newtheorem{problem}{Problem}
 \newtheorem{proposition}{Proposition}[section]
 \newtheorem{lemma}{Lemma}[section]
 \newtheorem{corollary}[theorem]{Corollary}
 \newtheorem{example}{Example}[section]
 \newtheorem{definition}[problem]{Definition}
 \newcommand{\BEQA}{\begin{eqnarray}}
 \newcommand{\EEQA}{\end{eqnarray}}
 \newcommand{\define}{\stackrel{\triangle}{=}}
 \bibliographystyle{IEEEtran}
 \providecommand{\mbf}{\mathbf}
 \providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
 \providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
 \providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
 \providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
 \providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
 \providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
 \providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
 \providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
 \providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
 \providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
 \providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
 \theoremstyle{remark}
 \newtheorem{rem}{Remark}
 \newcommand{\sgn}{\mathop{\mathrm{sgn}}}
 \providecommand{\abs}[1]{\left\vert#1\right\vert}
 \providecommand{\res}[1]{\Res\displaylimits_{#1}} 
 \providecommand{\norm}[1]{\left\lVert#1\right\rVert}
 %\providecommand{\norm}[1]{\lVert#1\rVert}
 \providecommand{\mtx}[1]{\mathbf{#1}}
 \providecommand{\mean}[1]{E\left[ #1 \right]}
 \providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
 %\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
 \providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
 	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
 \newcommand{\solution}{\noindent \textbf{Solution: }}
 \newcommand{\cosec}{\,\text{cosec}\,}
 \providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
 \newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
 \newcommand{\mydet}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
 \numberwithin{equation}{subsection}
 \makeatletter
 \@addtoreset{figure}{problem}
 \makeatother
 \let\StandardTheFigure\thefigure
 \let\vec\mathbf
 \renewcommand{\thefigure}{\theproblem}
 \newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
% Title content
\title{Assignment 1}
\author{Matish Singh Tanwar AI20MTECH11005\\Adarsh Srivastava AI20MTECH14008}
\begin{document}
\maketitle


% Introduction and Overview
\section{Question 1}
Consider a linear model of the form
\begin{align}
y(x,w)=w_0 + \sum w_ix_i
\end{align}
together with a sum-squares error function of the form
\begin{align}
E_D(w)=\frac{1}{2}\sum\{y(x_n,w)-t_n\}^2
\end{align}
Now suppose that Gaussian noise $\in_i$ with zero mean and variance $\sigma^2$ is added independently to each of the input variables $x_i$. By making use of E[$\in_i$] = 0 and E[$\in_i$$\in_j$] = $\delta_ij$$\sigma^2$, show that minimizing $E_D$ averaged over the noise distribution is equivalent to minimizing the sum-of-squares error for noise-free input variables with the addition of a weight-decay regularization term, in which the bias parameter $w_0$ is omitted from the regularizer.
% Example Subsection
\subsection{Solution}
According to the question, noise is added to each input variable individually, So,
\begin{align}
y'(x,w)=w_0 + \sum_{i=1}^{D} (w_ix_i+\in_i) \label{model}
\end{align}  
becomes our new model where $\in_i$ refers to the individual errors. Now, using \eqref{model}
\begin{align}
y'(x,w)=w_0 + \sum_{i=1}^{D} w_ix_i+\sum_{1}^{D} \in_i\\
y'(x,w)=y(x,w)+\sum_{i=1}^{D} \in_i\label{finalmodel}
\end{align}
Now, let's consider the effect of noise on the sum square error function,
\begin{align}
E_D'(w)=\frac{1}{2}\sum\{y'(x_n,w)-t_n\}^2\label{newerror}
\end{align}
Substituting \eqref{finalmodel} in \eqref{newerror},
\begin{align}
E_D'(w)=\frac{1}{2}\sum_{1}^{N}\{y(x,w)+\sum_{i=1}^{D}w_i\in_i-t_n\}^2\\
=\frac{1}{2}\sum_{1}^{N}\{(y(x,w))^2+\cleft(\sum_{i=1}^{D}w_i \in_i\cright)^2+t_n^{2}+2y(x,w)\sum_{i=1}^{D}w_i\in_i-2\sum_{i=1}^{D}w_i\in_i t_n-2t_ny(x,w)\}
\end{align}
Now, taking expectation of the new sum-of-squares 
\begin{align}
E[E_D'(w)]=\frac{1}{2}\sum_{1}^{N}\{(y(x,w))^2+E\cleft[\cleft(\sum_{i=1}^{D} w_i\in_i\cright)^2\cright]+t_n^{2}+2y(x,w)\sum_{i=1}^{D}w_iE[\in_i]-2\sum_{i=1}^{D}w_iE[\in_i] t_n-2t_ny(x,w)\}
\end{align}
Since E[$\in_i$]=0,
\begin{align}
E[E_D'(w)]=\frac{1}{2}\sum_{1}^{N}\{(y(x,w))^2+E\cleft[\cleft(\sum_{i=1}^{D} \in_i\cright)^2\cright]+t_n^{2}-2t_ny(x,w)\}\\
=\frac{1}{2}\sum_{1}^{N}\{(y(x,w))^2+\sum_{i=1}^{D}\sum_{i=1}^{D}w_iw_i'\in_i\in_i'+t_n^{2}-2t_ny(x,w)\}\\
=\frac{1}{2}\sum_{1}^{N}\{(y(x,w))^2+\sum_{i=1}^{D}\sum_{i=1}^{D}w_iw_i'E[\in_i\in_i']+t_n^{2}-2t_ny(x,w)\}\end{align}
Since $E[i_ii_i]=1$,
\begin{align}
=\frac{1}{2}\sum_{1}^{N}\{(y(x,w))^2+\sum_{i=1}^{D}w_i^2+t_n^{2}-2t_ny(x,w)\}\\
=\frac{1}{2}\sum_{1}^{N}\{(y(x,w)-t_n)^2+\sum_{i=1}^{D}w_i^2\}\\
=E_D(w)+\frac{N}{2}\sum_{i=1}^{D}w_i^2
\end{align}
Hence we get a Ridge Regression term without the bias parameter.

\section{Question 2}
Consider the problem where inputs are associated with multiple real valued outputs (K >
1) known as multi output regression (For e.g. predicting student score across different
courses).
\begin{align*}
y(x,w)=W^T(x)\phi(x)
\end{align*}
here y is a K-dimensional column vector, W is an M * K matrix of parameters,and $\phi$(x) is an M-dimensional column vector with elements $\phi$j(x), with $\phi$0(x) = 1.
\begin{enumerate}
\item{Provide the expression for the likelihood, and derive ML and MAP estimates of W in
the multi output regression case.}
\item{Consider a multi-output regression problem where we have multiple independent
outputs in linear regression. Let's consider a 2 dimensional output vector $y_i$ $\in$ $R^2$.
Suppose we have some binary input data, $x_i$ $\in$ {0, 1}. The training data is as given
in the right side. Let us embed each $x_i$ into 2d using the following basis function: $\phi$
(0) = $(1, 0)^T$, $\phi$(1) = $(0,1)^T$ . The model becomes y =$W^TÏ†(x)$ where W = [$w_1$, $w_2$] is a 2 x 2 matrix, with both $w_1$ and $w_2$ column vectors. Find the MLE for $w_1$ and $w_2$}
\end{enumerate}
\begin{center}
\begin{tabular}{l|c}
      x & y \\
      \hline
      0 & (-1,-1)^T \\
      0 & (-1,-2)^T \\
      0 & (-2,-1)^T \\
      1 & (1,1)^T \\
      1 & (1,2)^T \\
      1 & (2,1)^T \\
    \end{tabular}
    \end{center}
\subsection{Solution}
\begin{enumerate}
\item{
\subsection{Estimation of MLE}
\begin{align}
 t=y(x,w)+\epsilon
\end{align}
where $\epsilon$ is a zero mean gaussian random variable with variance $\sigma^2$\\
\begin{align}
 \epsilon \sim N(0,\sigma^2)
\end{align}
And we know that t labels are normally distributed.Our aim is to estimate the best values for mean and variance of normal distribution t. Let's get the mean and variance of t in terms of y and $\epsilon$ normal distribution. 
\begin{align}
 \mathbf{E}(t)=\mathbf{E}(y + \epsilon)\\
 \mathbf{E}(t)=\mathbf{E}(y) + \mathbf{E}(\epsilon)\\
 \mathbf{E}(t)=W^T\phi(x) + 0\\
 \mathbf{E}(t)=W^T\phi(x)
\end{align}
Similarly for variance,
\begin{align}
    \mathbf{Var}(t)=\mathbf{Var}(y + \epsilon)\\
    \mathbf{Var}(t)=\mathbf{Var}(y) + \mathbf{Var}(\epsilon)\\
    \mathbf{Var}(t)=0 + \sigma^2\\
    \mathbf{Var}(t)=\sigma^2
\end{align}
So,
\begin{align}
 t \sim \mathcal{N}(W^T\phi(x),\sigma^2)\\
 p(t|x,w,\sigma^2) = \mathcal{N}(t|W^T\phi(x),\sigma^2)
\end{align}
If we have a set of observations $t_1,....,t_n$ we can combine these into a matrix T of size NxK such that $n^{th}$ row is given by $t_n^T$.Similarly we can combine the input vectors $X1,...X_n$ into a matrix X. The log likelihood function is then given by:- 
\begin{align}
   \ln(p(t|x,w,\sigma^2)) = \sum_{n=1}^{N}\ln(\mathcal{N}(t_n|W^T\phi(x_n),\sigma^2))\\
   \implies -\frac{NK}{2}\ln(2\pi)-\frac{NK}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{n=1}^{N}\abs{t_n-W^T\phi(x_n)}^2
\end{align}
Differentiating above equation with respect to w
\begin{align}
    \frac{\partial (\ln(p(t|x,w,\sigma^2)))}{\partial w}=\frac{1}{2\sigma^2}\frac{\partial (T^2-2\phi(x_n)^TWT+\phi(x_n)^T\phi(x_n)W^2)}{\partial w}\\
    \implies\frac{1}{2\sigma^2}(0-2\phi(x_n)^T+2\phi(x_n)^T\phi(x_n)W)
\end{align}
Optimal values for W is when
\begin{align}
    \frac{\partial (\ln(p(t|x,w,\sigma^2)))}{\partial w}=0\\
    \implies 2\phi(x_n)^TT= 2\phi(x_n)^T\phi(x_n)W\\
    \implies \boxed{W=(\phi(x_n)^T\phi(x_n))^{-1}\phi(x_n)^TT}
\end{align}

\subsection{Estimation of MAP}
Conjugate pair of Normal Distribution is Normal Distribution.
\
p(w$|$x,t,$\alpha$,$\beta$) is the posterior.We have to calculate this.In this Prior probability will also be taken into consideration.
where $\beta$ is inverse variance used for Likelihood Normal Distribution.
\begin{align}
 p(w|x,t,\alpha,\beta)\propto p(t|x,w,\beta)p(w|\alpha)\\
 p(w|\alpha)= \mathcal{N}(w|0,\alpha^{-1}I)\\
 \implies (\frac{\alpha}{2\pi})^{(M+1)/2}\exp{(-\frac{\alpha}{2}w^Tw)}\\
\end{align}
Taking log on both sides
\begin{align}
    \ln(p(w|\alpha))=\frac{M+1}{2}\ln{\alpha}-\frac{M+1}{2}\ln{2\pi}-\frac{\alpha}{2}w^Tw
\end{align}
We got both prior as well as likelihood.Now combining them and taking the log of posterior also we get equation as
\begin{align}
   -\frac{\beta}{2}\sum_{n=1}^{N}\abs{t_n-W^T\phi(x_n)}^2 - \frac{\alpha}{2}w^Tw 
\end{align}
Now differentiating this with respect to w we get
\begin{align}
    -\frac{\beta}{2}(0-2\phi(x_n)^TT+2\phi(x_n)^T\phi(x_n)W)-\alpha W
\end{align}
Making it equal to 0 for optimal values,we get
\begin{align}
    \phi(x_n)^TT= (\phi(x_n)^T\phi(x_n)+\frac{\alpha}{\beta})W\\
\implies\boxed{W=(\frac{\alpha}{\beta}I+\phi(x_n)^T\phi(x_n))^{-1}\phi(x_n)^TT} 
\end{align}
For each target variable $t_k$ we get $w_k$ as
\begin{align}
   w_k=(\frac{\alpha}{\beta}I+\phi(x_n)^T\phi(x_n))^{-1}\phi(x_n)^Tt_k
\end{align}
}
\item{}
\subsection{As per the given question we can write,}
\begin{align}
\left[
  \begin{array}{ccc}
    y_1  \\
     y_2 
  \end{array}
\right]=\left[
  \begin{array}{ccc}
    \horzbar & w_1^{T} & \horzbar \\
    \horzbar & w_2^{T} & \horzbar 
  \end{array}
\right]
\left[
  \begin{array}{ccc}
    \phi_1 \\
     \phi_2 
  \end{array}
\right]
\end{align}
Now, From the table provided in the question,
\begin{align}
X=\left[
  \begin{array}{ccc}
     1 & 0 \\
     1 & 0 \\
     1 & 0 \\
     0 & 1 \\
     0 & 1 \\
     0 & 1 \\
  \end{array}
\right],
y_1=\left[
  \begin{array}{ccc}
     -1  \\
     -1  \\
     -2  \\
     1 \\
     1  \\
     2  \\
  \end{array}
\right],
y_2=\left[
  \begin{array}{ccc}
     -1  \\
     -2  \\
     -1  \\
     1 \\
     2  \\
     1  \\
  \end{array}
\right]
\end{align}
From equation one it is evident that multi-output problem is composed of following two single output linear regression.
\begin{align}
y_1=\phi(x)\^{w}_1\\
\^{W}_1=((X^TX)^{-1}X^T)y_1\label{y1}\\
y_2=\phi(x)\^{w}_2\\
\^{W}_2=((X^TX)^{-1}X^T)y_2\label{y2}
\end{align}
Calculating the value of $(X^TX)^{-1}X^T$,
\begin{align}
(X^TX)^{-1}X^T=\left[
  \begin{array}{cccccc}
     \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 &0 \\
    0 & 0 &0 &\frac{1}{3} & \frac{1}{3} & \frac{1}{3}
  \end{array}
\right]
\end{align}
Solving \eqref{y1} and \eqref{y2},
\begin{align}
\^{w}_1=((X^TX)^{-1}X^T)y_1\\
=\left[
  \begin{array}{cccccc}
     \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 &0 \\
    0 & 0 &0 &\frac{1}{3} & \frac{1}{3} & \frac{1}{3}
  \end{array}
\right]\left[
  \begin{array}{ccc}
     -1  \\
     -1  \\
     -2  \\
     1 \\
     1  \\
     2  \\
  \end{array}
\right]\\
=\begin{bmatrix}-\frac{4}{3}\\\frac{4}{3}\end{bmatrix}\\
\^{w}_2=((X^TX)^{-1}X^T)y_2\\
=\left[
  \begin{array}{cccccc}
     \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 &0 \\
    0 & 0 &0 &\frac{1}{3} & \frac{1}{3} & \frac{1}{3}
  \end{array}
\right]\left[
  \begin{array}{ccc}
     -1  \\
     -2  \\
     -1  \\
     1 \\
     2  \\
     1  \\
  \end{array}
\right]\\
=\begin{bmatrix}-\frac{4}{3}\\\frac{4}{3}\end{bmatrix}
\end{align}
As we can see that $\^{w}_1$ and $\^{w}_2$ are equal to $\begin{bmatrix}-\frac{4}{3}\\\frac{4}{3}\end{bmatrix}$
Hence,
$\^{W}$=$\begin{bmatrix}-\frac{4}{3}&-\frac{4}{3}\\\frac{4}{3}&\frac{4}{3}\end{bmatrix}$\\
\section{Question 3}
Model the horse kick deaths using the Poisson distribution with different parameters for each
of the corps. Learn Poisson distribution parameters for each of the corps using first 13 years
of data and make predictions on remaining 7 years and compute the RMSE of predictions for
each of the corps.
\begin{enumerate}
\item{Use maximum likelihood estimation to learn the parameters.}
\item{Use maximum aposteriori estimation to learn the parameters}
\begin{enumerate}
\item{Assume appropriate prior distribution over parameters and justify your assumption}
\item{Plot prior, likelihood and posterior and provide your observations in terms of mode
of the distributions for corps 2, 4 and 6.
\end{enumerate}
\end{enumerate}}
\subsection{Solution}
\begin{enumerate}
\item{\lstset{language=Python}
In this part we have to estimate the parameters using MLE and predict the values for the next 7 years. So, first we are calculating MLE for each corps as shown in the following code.
\lstset{frame=lines}
\lstset{caption={Parameter Through MLE }}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
import numpy
import matplotlib.pyplot as plt
import math
'Creating training and test datasets'
train=numpy.array([[0,2,2,1,0,0,1,1,0,3,0,2,1],[0,0,0,1,0,3,0,2,0,0,0,1,1],[0,0,0,2,0,2,0,0,1,1,0,0,2],[0,0,0,1,1,1,2,0,2,0,0,0,1],
[0,1,0,1,1,1,1,0,0,0,0,1,0],[0,0,0,0,2,1,0,0,1,0,0,1,0],[0,0,1,0,2,0,0,1,2,0,1,1,3],[1,0,1,0,0,0,1,0,1,1,0,0,2],[1,0,0,0,1,0,0,1,0,0,0,0,1],
[0,0,0,0,0,2,1,1,1,0,2,1,1],[0,0,1,1,0,1,0,2,0,2,0,0,0],[0,0,0,0,2,4,0,1,3,0,1,1,1],[1,1,2,1,1,3,0,4,0,1,0,3,2],[0,1,0,0,0,0,0,1,0,1,1,0,0]])
test=numpy.array([[0,0,1,0,1,0,1],[1,0,2,0,3,1,0],[1,1,0,0,2,0,0],[0,1,2,1,0,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,0],
                 [1,1,1,0,3,0,0],[0,0,2,1,0,2,0],[0,0,0,1,1,0,1],[0,1,2,0,1,0,0],[0,2,1,3,0,1,1],[1,2,1,3,1,3,1],
                 [1,0,2,1,1,0,0],[0,2,2,0,0,0,0]])
'Using MLE to learn the parameters'
MLE_parameter=[sum(i) for i in train]
MLE_parameter=[i/13 for i in MLE_parameter]
print("MLE Parameters for Corps:")
print(MLE_parameter)
plt.plot(MLE_parameter)
plt.title("MLE Parameter")
plt.show()
\end{lstlisting}
In this portion we are predicting the values of next 7 years by using MLE of the previous 13 years. MLE is depecting the average number of deaths per year and that average death rounded off can be considered as the estimation for death per year.
\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Predicted Values }}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
''Now predicting values for last 7 years'
prediction=[round(i) for i in MLE_parameter]
print("Predicted Value for next 7 years for each corp")
predictedval=[i*7 for i in prediction]
print(predictedval)
\end{lstlisting}
Finally, We are calculating the Root Mean Square Error by using the formula 
\begin{align}
RMSE=\sqrt{\frac{1}{n}\Sigma_{i=1}^{n}{\Big(\frac{d_i -f_i}{\sigma_i}\Big)^2}}
\end{align}
\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Root Mean Square Error }}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
'Calculating RMSE'
values=[]
for i in range(0,len(test)):
    value=[]
    for j in range(0,7):
        value.append(prediction[i])
    values.append(value)
diff=[]
for i in range(0,len(test)):
    diff.append(numpy.subtract(values[i],test[i]))
sum=[]
diff=numpy.array(diff)
data=[0,0,0,0,0,0,0,0,0,0,0,0,0,0]
for i in range(0,len(test)):
        data[i]=[i*i for i in diff[i]]
for i in range(0,len(test)):
    sum.append(numpy.sum(data[i]))
sum=numpy.array(sum)
MSE=[i/7 for i in sum]
RMSE=[numpy.sqrt(i) for i in MSE]
print("RMSE for Corps")
print(RMSE)
plt.plot(RMSE)
plt.title("Root Mean Square Error")
plt.show()
print("Average RMSE for Corps")
print(numpy.sum(RMSE)/len(RMSE))
\end{lstlisting}}
\begin{enumerate}
\item{Results}
MLE for the corps came out to be,
\begin{align}
x_1=1.0\\x_2=0.6153846153846154\\x_3=0.6153846153846154\\x_4=0.6153846153846154\\x_5= 0.46153846153846156\\x_6=0.38461538461538464\\x_7=0.8461538461538461\\x_8=0.5384615384615384\\x_9=0.3076923076923077\\x_{10}=0.6923076923076923\\x_{11}=0.5384615384615384\\x_{11}=1.0\\x_{12}=1.4615384615384615\\x_{13}=0.3076923076923077
\end{align}
Prediction per year is the rounded off value of these MLE.
RMSE for the corps came out to be,
\begin{align}
x_1=0.7559289460184544\\x_2=1.0690449676496976\\x_3=0.8451542547285166\\x_4=0.8451542547285166\\x_5=0.5345224838248488\\x_6=0.9258200997725514\\x_7=1.0\\x_8=0.9258200997725514\\x_9=0.6546536707079771\\x_{10}=0.8451542547285166\\x_{11}=1.0\\x_{11}=1.1338934190276817\\x_{12}= 0.7559289460184544\\x_{13}=1.0690449676496976\\
Avg_{RMSE}=0.8828657403305332
\end{align}
\item{Plots}\\
This portion consist of visual representation of MLE parameters and RMSE of each individual corps.
\begin{figure}[h!]
	\includegraphics[width=\columnwidth]{MLE.png}
	\caption{}
	\label{myfig}
\end{figure}
\begin{figure}[h!]
	\includegraphics[width=\columnwidth]{RMSE.png}
	\caption{}
	\label{myfig2}
\end{figure}
\end{enumerate}
\item{\lstset{language=Python}
In this section we are using MAP to estimate the parameters. We are taking Gamma as the prior in the following code.An explanation of why gamma is a suitable prior will be present at the end of the question.
\lstset{frame=lines}
\lstset{caption={Plotting Prior,Likelihood and Posterior}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
def mode(values):
    values.sort() 
    temp=[] 
    i = 0
    while i < len(values) : 
        temp.append(values.count(values[i])) 
        i += 1
    temp1 = dict(zip(values, temp)) 
    temp2={k for (k,v) in temp1.items() if v == max(temp) } 
    for x in temp2:
        Mode=x
    return Mode
alpha = [1, 0.9, 1]
scale = [0.75, 2, 4]
dataset=[[0,0,0,2,0,2,0,0,1,1,0,0,2],[0,1,0,1,1,1,1,0,0,0,0,1,0],[0,0,1,0,2,0,0,1,2,0,1,1,3]]
for i in range(0,3):
  Mode=mode(dataset[i])
  mean=np.mean(dataset[i])
  var=np.var(dataset[i])
  x = Mode
  x=np.linspace(0,4,100)
  y1 = gamma.pdf(x, a=alpha[i], scale=scale[i])
  plt.plot(x, y1)
  plt.title("Corp prior vs Mode")
  plt.show()
  MAP=(alpha[i]-1+mean)
  MAP=MAP/((1/scale[i])+1)
  plt.axvline(MAP, ymin = 0, ymax=1, linewidth = 2)
  plt.title("MAE")
  plt.show()
  plt.plot(x, MAP*y1, "r-", )
  plt.title("Corp posterior vs Mode")
  plt.show()
\end{lstlisting}
\end{enumerate}
$\textbf{Gamma prior is conjugate to Poisson}$
\begin{align}
    X_i\sim P(\lambda)
\end{align}
X follows poisson distribution with mean $\lambda$
\begin{align}
  P(X_i|\lambda)=\frac{{e^{ - \lambda } \lambda ^{X_i} }}{{X_i!}}\\
  \lambda \sim gamma(\alpha,\beta)\\
  P(\lambda)=\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta\lambda}\\
  \propto \lambda^{\alpha-1}e^{-\beta\lambda}\label{1}\\
  P(\lambda|X)=\frac{P(X|\lambda)P(\lambda)}{P(X)}\\
  \implies \propto P(X|\lambda)P(\lambda)\label{2}\\
  P(X|\lambda)=\prod_{i=1}^{n}\frac{{e^{ - \lambda } \lambda ^{X_i}}}{{X_i!}}\\
  \implies \frac{e^{-N\lambda}\lambda^{X_1+X_2+...+X_N}}{\prod_{i=1}^{n}X_i!}\\
  \implies \propto e^{-N\lambda}\lambda^{X_1+X_2+...+X_N}\\
  \propto e^{-N\lambda}\lambda^{N\overline{X}}\label{3}
\end{align}
Substituting \eqref{3},\eqref{1} in \eqref{2} we get,
\begin{align}
    P(\lambda|X)\propto e^{-N\lambda}\lambda^{N\overline{X}} \lambda^{\alpha-1}e^{-\beta\lambda}\\
    \implies \lambda^{N\overline{X}+\alpha-1}e^{-(N+\beta)\lambda}\\
    \sim gamma(N\overline{X}+\alpha,\beta+N)\label{4}
\end{align}
From \eqref{4} it can be seen that we have prooved that Gamma Prior is conjugate to Poisson

\section{Question 4}
You are provided hourly rental data spanning two years (Data (training and test) available here). The training set is
comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict
the total count of bikes rented during each hour covered by the test set, using only information available prior to the
rental period. Fit a Poisson regression model to the count data (output). Treat year, month, weekday, hour, holiday,
weather, atemp, humidity, windspeed etc. as input features that are combined linearly to determine the rate
parameter of the Poisson distribution. Create a 80-20 split of the train data into training, and validation.
\begin{enumerate}
\item{Explain maximum likelihood estimation in poisson regression and derive the loss function which is
used to estimate the parameters.}
\item{Find statistics of the dataset like mean count per year, month etc.}
\item{Plot count against any 5 features.}
\item{Apply L1 and L2 norm regularization over weight vectors, and find the best hyper-parameter
settings for the mentioned problem using validation data and report the accuracy on test data for
no regularization, L1 norm regularization and L2 norm regularization.}
\item{Determine most important features determining count of bikes rented.}
\end{enumerate}
\subsection{Solution}
\begin{enumerate}
\item{Maximum Likelihood Estimation}\\
Poisson regression model is a generalized linear model with poisson error and a log link function. In poisson regression the parameters can be estimated by using Maximum Likelihood Estimate which is of the form,
\begin{align}
L(\lambda,y)=\Pi_{i=1}^{n}\left[e^{-\lambda_i}\frac{{\lambda_i}^{y_i}}{y_i!}\right]\label{poisson}
\end{align}
where,
\begin{align}
\lambda=(\lambda_1,\lambda_2,\cdots)^T\\
y=(y_1,y_2,\cdots)^T
\end{align}
Now taking log of \eqref{poisson} and using $\lambda_i=e^{x_{i}^{T}\beta}$,
\begin{align}
l(\lambda,y)=-\sum_{i}^{n}exp(e^{x_{i}^{T}\beta})+\sum_{i}^{n}y_iln(exp(e^{x_{i}^{T}\beta}))-\sum_{i}^{n}ln(y!)
\end{align}
The above equation represents the log likelihood of poisson regression. Differentiating it respect to $\beta$ and using the iterative weighted least square estimation,
\begin{align}
\beta=(X^TWX)^{-1}X^TWq
\end{align}
where W=$diag(\lambda_1,\lambda_2,\lambda_3...)$
\item{Statistics of the dataset}
\end{enumerate}
\end{enumerate}
\end{document}
